{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07877fe4-bc31-459f-a837-bb4c15f3d416",
   "metadata": {},
   "source": [
    "# Correspondence Learning\n",
    "\n",
    "Exercise of sound-image correspondence learning.\n",
    "It is introduced in the paper:\n",
    "\n",
    "> D. Harwath, A. Torralba, and J. Glass, \"Unsupervised learning of spoken language with visual context,\" in *Proc. NIPS*, 2016.\n",
    "\n",
    "## Table of contents\n",
    "1. Remove silence from segmented audio\n",
    "1. Extract spectrogram from segmented audio\n",
    "1. Add S/N=30dB white noise to segmented audio\n",
    "1. Dataset\n",
    "1. Loss function\n",
    "1. Model\n",
    "1. Train model\n",
    "1. Extract features using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ff50b-72b0-4221-a76b-39c383824508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "from glob import glob\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pydub import AudioSegment\n",
    "import resampy\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe3beb-1f94-4f2a-b2cc-0084c55aa44d",
   "metadata": {},
   "source": [
    "## Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756effa8-bb11-4e31-9b30-e727b22eb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Remove silence\n",
    "        self.load_dir = \"exp1/seg/segmented_wavs\"\n",
    "        self.save_dir = \"exp1/unsup_rl/test/trimed_wavs\"\n",
    "        # Extract spectrogram\n",
    "        self.audio_list = \"exp1/unsup_rl/test/segmented_wavs.txt\"\n",
    "        self.save_list = \"exp1/unsup_rl/test/mfccs.txt\"\n",
    "        # Add noise\n",
    "        self.noisy_dir = \"exp1/unsup_rl/test/noisy_trimed_wavs\"\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be1fb4-9609-44aa-b6eb-06278b034f31",
   "metadata": {},
   "source": [
    "## Remove silence from segmented audio\n",
    "\n",
    "As a preprocessing, we remove silence from the audio segments and add noise to them.\n",
    "Remove silence from segmented audio because a combined wave file contains 1-3 seconds of random intervals.\n",
    "\n",
    "<img src=\"../fig/data.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9aed-9381-42f2-9a5f-1d31bd71e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(202006241517)\n",
    "\n",
    "def get_sampling_rate(audio_path: str) -> int:\n",
    "    import wave\n",
    "    with wave.open(audio_path, \"rb\") as f:\n",
    "        return f.getframerate()\n",
    "\n",
    "def rm_voiceless(audio_path: str, save_path: str):\n",
    "    sr = get_sampling_rate(audio_path)\n",
    "    sound, _ = librosa.core.load(audio_path, sr = sr)\n",
    "    sound, _ = librosa.effects.trim(sound)\n",
    "    # sf.write(save_path, sound ,sr)\n",
    "\n",
    "audio_dir = args.load_dir\n",
    "save_dir = args.save_dir\n",
    "\n",
    "paths = [i for i in Path(audio_dir).glob(\"*.wav\")]\n",
    "for p in tqdm(paths, desc=\"rm_voiceless\"):\n",
    "    savep = Path(save_dir) / p.name\n",
    "    rm_voiceless(str(p), str(savep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd5093-4466-4883-bdc1-fbc6ecddf360",
   "metadata": {},
   "source": [
    "## Extract spectrogram from segmented audio for A_Dataset\n",
    "\n",
    "A_Dataset will be defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618568e-650b-4370-b538-7d3468004b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc(a, sr):\n",
    "    win_len_sec = 0.025\n",
    "    hop_sec = 0.010\n",
    "    ret = librosa.stft(a, n_fft=int(win_len_sec * sr),\n",
    "                        hop_length=int(hop_sec * sr))\n",
    "    return ret\n",
    "\n",
    "def extract_and_save_from_mp3file(io):\n",
    "    in_path, out_path = io\n",
    "    if Path(out_path).is_file(): return\n",
    "    try:\n",
    "        feature = mp3file_to_examples(in_path)\n",
    "        # with open(out_path, \"wb\") as f:\n",
    "        #     pickle.dump(feature, f)\n",
    "    except Exception as e:\n",
    "        print(\"Failed: {}\".format(in_path), file=sys.stderr)\n",
    "\n",
    "def mp3file_to_examples(mp3_file):\n",
    "    a, sr = librosa.load(mp3_file)\n",
    "    resr = 16000\n",
    "    a = resampy.resample(a, sr, resr)\n",
    "    feature = enc(a, resr)\n",
    "    return feature\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_list = []\n",
    "    with open(args.audio_list, \"r\") as f:\n",
    "        for p in f:\n",
    "            audio_list.append(p.strip())\n",
    "\n",
    "    save_list = []\n",
    "    with open(args.save_list, \"r\") as f:\n",
    "        for p in f:\n",
    "            save_list.append(p.strip())\n",
    "        \n",
    "    plist = list(zip(audio_list, save_list))\n",
    "\n",
    "    from multiprocessing import Pool\n",
    "    import multiprocessing as multi\n",
    "    import sys\n",
    "\n",
    "    pool = Pool(multi.cpu_count())\n",
    "    for i, _ in enumerate(pool.imap_unordered(extract_and_save_from_mp3file, plist)):\n",
    "        if i % 1000 == 0: print(i, flush=True)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92132c-ad76-4e5d-a12d-a7d33d2ca8f9",
   "metadata": {},
   "source": [
    "## Add S/N=30dB white noise to segmented audio\n",
    "\n",
    "This is the noise in the agent-human interaction environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49707f6-b1c5-4d3f-ae9d-f54464649789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_voiceless_and_noise(audio_path: str, save_path: str, noise_db: int = 30, voiceless_len_ms: int = 1000):\n",
    "    from pydub.generators import WhiteNoise\n",
    "    sound = AudioSegment.from_wav(audio_path)\n",
    "    noise_dbfs = sound.dBFS-noise_db\n",
    "    \n",
    "    def make_sil(duration: int) -> AudioSegment:\n",
    "        return AudioSegment.silent(duration=duration)\n",
    "    \n",
    "    def make_noise(duration: int, dbfs: float) -> AudioSegment:\n",
    "        return WhiteNoise().to_audio_segment(duration=duration, volume=dbfs)\n",
    "\n",
    "    noise = make_noise(len(sound), noise_dbfs)\n",
    "    sound = sound.overlay(noise)\n",
    "    \n",
    "    first_noise = make_noise(voiceless_len_ms, noise_dbfs)\n",
    "    last_noise  = make_noise(voiceless_len_ms, noise_dbfs)\n",
    "    sound = first_noise + sound + last_noise\n",
    "    sound.export(save_path, format=\"wav\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(202006241517)\n",
    "\n",
    "    audio_dir = args.save_dir\n",
    "    save_dir = args.noisy_dir\n",
    "    paths = [i for i in Path(audio_dir).glob(\"*.wav\")]\n",
    "    for p in tqdm(paths, desc=\"Add white noise\"):\n",
    "        savep = Path(save_dir) / p.name\n",
    "        # add_voiceless_and_noise(p, savep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18f6e9-451c-4bf8-b3a0-cfc0c095aa40",
   "metadata": {},
   "source": [
    "## Example of segmented audio\n",
    "1. with silence\n",
    "1. without silence\n",
    "1. with S/N=30dB white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ce370-3a57-4141-b2a5-2e1736c27999",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_with_silence, rate = librosa.load(\"exp1/seg/segmented_wavs/1_7438.wav\")\n",
    "Audio(audio_with_silence, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a2088-0292-40d0-bf08-a4919545fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_without_silence, rate = librosa.load(\"exp1/unsup_rl/test/trimed_wavs/1_7438.wav\")\n",
    "Audio(audio_without_silence, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9db2a7-c892-47e7-8bac-134b2deac931",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_audio, rate = librosa.load(\"exp1/unsup_rl/test/noisy_trimed_wavs/1_7438.wav\")\n",
    "Audio(noisy_audio, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dccabf-ab2f-4ab4-949e-d8f467ebe6ce",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- AV_Dataset: Tuple of audio description and food image. For each food, we used 90 images for training. There are four types of audio descriptions.\n",
    "    - foodname\n",
    "    - A (An) foodname\n",
    "    - A (An) color foodname\n",
    "    - It's a (an) foodname\n",
    "\n",
    "- V_Dataset: training images in AV_Dataset.\n",
    "\n",
    "- A_Dataset: Spectrogram of segmented audio including meaningless sound.\n",
    "\n",
    "<img src=\"../fig/corr.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a4dfb-6ef7-44f7-83d6-94b4b5aa53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AV_Dataset(Dataset):\n",
    "    def __init__(self, pair_paths, a_tr, v_tr):\n",
    "        self.pair_paths = pair_paths  # (audio, visual)\n",
    "        self.a_tr = a_tr\n",
    "        self.v_tr = v_tr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_paths)\n",
    "\n",
    "    def _a_load(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            feat = pickle.load(f)\n",
    "        log_pad = 0.010\n",
    "        feat = np.log(np.abs(feat) + log_pad).T\n",
    "        if self.a_tr:\n",
    "            feat = self.a_tr(feat)\n",
    "        return feat\n",
    "\n",
    "    def _v_load(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((224, 224))\n",
    "        if self.v_tr:\n",
    "            img = self.v_tr(img)\n",
    "        return img\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)  # Cache loaded structures\n",
    "    def __getitem__(self, index):\n",
    "        a = self._a_load(self.pair_paths[index][0])\n",
    "        v = self._v_load(self.pair_paths[index][1])\n",
    "        return a, v\n",
    "\n",
    "class A_Dataset(Dataset):\n",
    "    def __init__(self, paths, a_tr, v_tr):\n",
    "        self.paths = paths  # (audio, visual)\n",
    "        self.a_tr = a_tr\n",
    "        self.v_tr = v_tr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _a_load(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            feat = pickle.load(f)\n",
    "        log_pad = 0.010\n",
    "        feat = np.log(np.abs(feat) + log_pad).T\n",
    "        if self.a_tr:\n",
    "            feat = self.a_tr(feat)\n",
    "        return feat\n",
    "\n",
    "    def _v_load(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((224, 224))\n",
    "        if self.v_tr:\n",
    "            img = self.v_tr(img)\n",
    "        return img\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)  # Cache loaded structures\n",
    "    def __getitem__(self, index):\n",
    "        a = self._a_load(self.paths[index])\n",
    "        return a\n",
    "\n",
    "class V_Dataset(Dataset):\n",
    "    def __init__(self, paths, a_tr, v_tr):\n",
    "        self.paths = paths  # (audio, visual)\n",
    "        self.a_tr = a_tr\n",
    "        self.v_tr = v_tr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _a_load(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            feat = pickle.load(f)\n",
    "        log_pad = 0.010\n",
    "        feat = np.log(np.abs(feat) + log_pad).T\n",
    "        if self.a_tr:\n",
    "            feat = self.a_tr(feat)\n",
    "        return feat\n",
    "\n",
    "    def _v_load(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((224, 224))\n",
    "        if self.v_tr:\n",
    "            img = self.v_tr(img)\n",
    "        return img\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)  # Cache loaded structures\n",
    "    def __getitem__(self, index):\n",
    "        v = self._v_load(self.paths[index])\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dca886-0387-430d-aa21-ade1e9c7deb0",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The sim_loss is based on the triplet loss.\n",
    "The imposter of the audio description is randomly selected from the rest of the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27e8e-139a-4229-8917-a54af3d7a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_index(ban_i, ceil_i):\n",
    "    max_i = ceil_i - 1\n",
    "    assert 0 <= ban_i <= max_i\n",
    "    i = random.randint(0, max_i - 1)\n",
    "    if i == ban_i:\n",
    "        i = max_i\n",
    "    return i\n",
    "\n",
    "\n",
    "def sim_loss_batchsum(feat_vs, feat_as, rho = 4.0, eps = 1e-5):\n",
    "    ret = 0\n",
    "\n",
    "    for i in range(len(feat_vs)):\n",
    "        j = random_index(i, len(feat_vs))\n",
    "        ret += sim_loss(feat_vs[i], feat_as[i], feat_vs[j], feat_as[j], rho, eps)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sim_loss(feat_v, feat_a, feat_v_imp,feat_a_imp, rho = 4.0, eps=1e-5):\n",
    "    assert len(feat_v) == len(feat_a)\n",
    "\n",
    "    score_corr = ((feat_v - feat_a)**2).sum()**(1/2)\n",
    "    score_impa = ((feat_v - feat_a_imp)**2).sum()**(1/2)\n",
    "    if score_corr.is_cuda:\n",
    "        zeros = torch.cuda.FloatTensor((1,)).fill_(0)\n",
    "    else:\n",
    "        zeros = torch.zeros(*score_corr.shape)\n",
    "    ret = 0.5*score_corr**2 + 0.5*torch.max(zeros, rho - score_impa)**2\n",
    "    ret = torch.mean(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd40fb9-1a54-4b7c-bcee-7ba3698ea934",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The sound front-end and image front-end are randomly initialized ResNet50.\n",
    "> K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" in *Proc. CVPR*, 2016.\n",
    "\n",
    "Using the model, audio and images are embedded in the same dimensional space.\n",
    "\n",
    "<img src=\"../fig/corr.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69078ac-8660-4a92-bd16-5b72cfc1c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImageCorrNet(nn.Module):\n",
    "    def __init__(self, num_class = 50, ext_v = True, ext_a = True, pre_trained=False):\n",
    "        super(SimpleImageCorrNet, self).__init__()\n",
    "        self.visual_net = models.resnet50(num_classes=num_class, pretrained=pre_trained)\n",
    "        self.audio_net = models.resnet50(num_classes=num_class, pretrained=pre_trained)\n",
    "        self.audio_net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.ext_v = ext_v\n",
    "        self.ext_a = ext_a\n",
    "\n",
    "    def _visual_extract(self, v):\n",
    "        return self.visual_net(v)\n",
    "\n",
    "    def _sound_extract(self, a):\n",
    "        return self.audio_net(a[:,None,:,:])\n",
    "\n",
    "    def forward(self, visual_feat, audio_feat):\n",
    "        if self.ext_v:      visual_feat = self._visual_extract(visual_feat)\n",
    "        if self.ext_a:      audio_feat = self._sound_extract(audio_feat)\n",
    "        return (visual_feat, audio_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25ed0d-da8a-467c-b4fb-7b52a955b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    def __init__(self, modality, pkllist, outpath):\n",
    "        # Correspondence learning\n",
    "        self.dataset_path = \"exp1/unsup_rl/test/audio_image_pair.txt\"\n",
    "        self.workdir = \"exp1/unsup_rl/test\"\n",
    "        # Conf\n",
    "        self.del_last = 1\n",
    "        self.ext_fusion = 0\n",
    "        self.rho = 2.0\n",
    "        self.epoch = 2\n",
    "        self.batch_size = 16\n",
    "        # Extract nn feat\n",
    "        self.modality = modality\n",
    "        self.modelparam = \"exp1/unsup_rl/test/unsup_backend.pth.tar\"\n",
    "        self.pkllist = pkllist\n",
    "        self.outpath = outpath\n",
    "\n",
    "\n",
    "opts = Opts(\n",
    "    \"audio\",\n",
    "    \"exp1/unsup_rl/test/seg_audios.txt\",\n",
    "    \"exp1/unsup_rl/test/data/new720db20k2_fea_sim_8type_clean_limited_data.npy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc961ed-e9bd-4d4b-8922-f389a40a1d85",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a39a6-ae77-44b0-b8bf-a6197b76db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_NAME=opts.workdir + \"/log/\" + datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y%m%d_%H_%M_%S\") + \".log\"\n",
    "Path(LOG_NAME).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def logger(s):\n",
    "    print(s, flush=True)\n",
    "    with open(LOG_NAME, \"a\") as f:\n",
    "        f.write(s)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "class KeepUnderThr(object):\n",
    "    def __init__(self, times, thr):\n",
    "        self.times = times\n",
    "        self.thr = thr\n",
    "        self.count = 0\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if loss < self.thr:\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.count = 0\n",
    "        return self.count >= self.times\n",
    "\n",
    "\n",
    "def get_remainder(datasize, b_batch_size, batch_size):\n",
    "    assert b_batch_size % batch_size == 0\n",
    "    remainder = datasize % b_batch_size\n",
    "    rem_start_iter = (datasize // b_batch_size) * (b_batch_size // batch_size)\n",
    "    return remainder, rem_start_iter\n",
    "\n",
    "\n",
    "def train_video_corr_net(model, device, loader, optimizer, epoch, start_time, break_fn):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    batch_num = 0\n",
    "    iter_num = 0\n",
    "    NUM_B_BATCH = opts.batch_size*10\n",
    "    rem, rem_start_iter = get_remainder(len(loader.dataset), NUM_B_BATCH, loader.batch_size)\n",
    "    for batch_idx, (snd, img) in enumerate(loader):\n",
    "        img, snd = img.to(device), snd.to(device)\n",
    "        loss = 0\n",
    "        if batch_num == 0:\n",
    "            iter_mean_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "        vcs, acs = model(img, snd)\n",
    "        loss = sim_loss_batchsum(vcs,acs,rho=opts.rho)\n",
    "        if batch_idx < rem_start_iter:\n",
    "            loss /= NUM_B_BATCH\n",
    "        else:\n",
    "            loss /= rem\n",
    "        iter_mean_loss += loss.item()\n",
    "        batch_num += img.shape[0]\n",
    "        loss.backward()\n",
    "        if batch_idx+1 != len(loader) and batch_num != NUM_B_BATCH: continue\n",
    "        iter_num += 1\n",
    "        sum_loss += iter_mean_loss\n",
    "        optimizer.step()\n",
    "\n",
    "        logger(f\"Train Epoch: {epoch} [{(batch_idx+1)*len(img)}/{len(loader.dataset)} ({int(100.*(batch_idx+1)/len(loader)):3}%)]\\tLoss: {iter_mean_loss:.6f}\\tTime:{time.time() - start_time}\")\n",
    "        batch_num = 0\n",
    "    loss_mean = sum_loss/iter_num\n",
    "    logger(\"Train Epoch: {} Loss: {:.6f}\\tTime:{}\".format(\n",
    "            epoch, loss_mean,time.time() - start_time))\n",
    "    return loss_mean\n",
    "\n",
    "\n",
    "def repeat_last(batch):\n",
    "    import numpy as np\n",
    "    a, v = [], []\n",
    "    a_len = -1\n",
    "    for b in batch:\n",
    "        a.append(b[0])\n",
    "        v.append(b[1])\n",
    "        a_len = max(a_len, len(b[0]))\n",
    "    a_pad = []\n",
    "    for ae in a:\n",
    "        pad = ae[-1:].repeat(a_len-len(ae), axis=0)\n",
    "        ae = np.concatenate((ae, pad), axis=0)\n",
    "        a_pad.append(torch.from_numpy(ae))\n",
    "    a_pad = torch.stack(a_pad, dim=0).float()\n",
    "    v = torch.stack(v, dim=0).float()\n",
    "    return a_pad,v\n",
    "\n",
    "\n",
    "def train_data2(shuffle=True, dataset_path=\"data/plist_train_noise.txt\"):\n",
    "    from torch.utils.data import DataLoader\n",
    "    paths = []\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        for t in f:\n",
    "            paths.append(tuple(t.strip().split()))\n",
    "    v_tr = transforms.ToTensor()\n",
    "    v_tr_train = transforms.ToTensor()\n",
    "    loader = DataLoader(AV_Dataset(paths, None, v_tr_train), batch_size=opts.batch_size, shuffle=shuffle, num_workers=7, pin_memory=True, collate_fn=repeat_last)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def saves(model, optimizer, path):\n",
    "    stt = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optim_state_dict\": optimizer.state_dict()\n",
    "            }\n",
    "    torch.save(stt, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f097fdf-7f12-4802-8028-d5ec518fe9de",
   "metadata": {},
   "source": [
    "If `opts.epoch` is set to a sufficiently large value, the training will automatically terminate after about 600-800 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e77d58-a154-4289-b438-c77d3e80fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    LOAD_EPOCH=0\n",
    "    END_EPOCH=opts.epoch\n",
    "    loader = train_data2(dataset_path=opts.dataset_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ext_fusion = True if opts.ext_fusion == 1 else False\n",
    "    del_last = True if opts.del_last == 1 else False\n",
    "    model = SimpleImageCorrNet(num_class = 50).to(device)\n",
    "    lr = 1e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    start_time = time.time()\n",
    "    break_fn = KeepUnderThr(times=10, thr=1e-5)\n",
    "    loss_list = []\n",
    "    patience = 0\n",
    "    loss_min = 100000\n",
    "    for epoch in range(LOAD_EPOCH+1, END_EPOCH+1):\n",
    "        loss_h = train_video_corr_net(model, device, loader, optimizer, epoch, start_time, break_fn)\n",
    "        len_loader = len(loader)\n",
    "        loss_list.append(loss_h)\n",
    "        # np.save(opts.workdir + \"/unsup_loss.npy\", loss_list)\n",
    "        loss_min = min(loss_list)\n",
    "\n",
    "        if loss_h > loss_min:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "            # saves(model, optimizer, f\"{opts.workdir}/unsup_backend.pth.tar\")\n",
    "            # print(\"save\")\n",
    "        if patience > 20*7200//opts.batch_size//len_loader and lr > 1e-7:\n",
    "            lr = lr/10\n",
    "            patience = 0\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            print(\"Decrease lr\")\n",
    "        if patience > 100*7200//opts.batch_size//len_loader:\n",
    "            print(\"Early stop\", loss_min)\n",
    "            break\n",
    "        print(loss_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321f4db-8b94-4636-8207-5f9147dc2620",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract features using pretrained model\n",
    "\n",
    "Using the model pretrained by correspondence learning, we extract features from V_Dataset and A_Dataset.\n",
    "The K-Means clustering and selection of audio segments by distance matrix are done in [exercise3.ipynb](exercise3.ipynb).\n",
    "\n",
    "<img src=\"../fig/corr.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65949940-6e1e-4102-afa1-cedb8585b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(parampath, modality, device):\n",
    "    model = SimpleImageCorrNet(num_class = 50).to(device)\n",
    "    checkpoint = torch.load(parampath)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    if modality == \"audio\":\n",
    "        return model._sound_extract\n",
    "    elif modality == \"image\":\n",
    "        return model._visual_extract\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "def load_data(dataset_path, modality):\n",
    "    paths = []\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        for t in f:\n",
    "            paths.append(t.strip())\n",
    "    v_tr = transforms.ToTensor()\n",
    "    if modality == \"audio\":\n",
    "        DatasetClass = A_Dataset\n",
    "    elif modality == \"image\":\n",
    "        DatasetClass = V_Dataset\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    seg_loader = DataLoader(DatasetClass(paths, None, v_tr), batch_size=1, shuffle=False, num_workers=7, pin_memory=True)\n",
    "    return seg_loader\n",
    "\n",
    "def extract(model, data, device):\n",
    "    seg_fea = []\n",
    "    with torch.no_grad():\n",
    "        for seg in tqdm(data):\n",
    "            seg = seg.to(device)\n",
    "            fea = model(seg)\n",
    "            seg_fea.append(fea.cpu())\n",
    "    seg_fea_tensor = torch.stack(seg_fea, dim=0)\n",
    "    return seg_fea_tensor\n",
    "\n",
    "\n",
    "def extract_nnfeat(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = load_model(args.modelparam, args.modality, device)\n",
    "    seg_loader = load_data(args.pkllist, args.modality)\n",
    "    seg_fea_tensor = extract(model, seg_loader, device)\n",
    "\n",
    "    # np.save(args.outpath, seg_fea_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacb62c-2641-4e44-954e-deb3bb048a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_nnfeat(opts)\n",
    "\n",
    "opts = Opts(\n",
    "    \"image\",\n",
    "    \"exp1/unsup_rl/test/train_imgs.txt\",\n",
    "    \"exp1/unsup_rl/test/data/train_img_fea_sim_8type_clean_limited_data.npy\",\n",
    ")\n",
    "extract_nnfeat(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59087d-4a6f-4ef7-ac26-f6fd011126bf",
   "metadata": {},
   "source": [
    "## Test pretrained model\n",
    "\n",
    "Plot the distance matrix between audio features and image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72e886-febb-4809-b0fd-77f7d0f21f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parampath = \"exp1/unsup_rl/test/unsup_backend.pth.tar\"\n",
    "model = SimpleImageCorrNet(num_class = 50).to(device)\n",
    "checkpoint = torch.load(parampath)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "loader = iter(train_data2(dataset_path=opts.dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed7204-c78f-42bc-a659-a0e1122b7b7a",
   "metadata": {},
   "source": [
    "## Function for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c894f-5f68-4eae-b6da-a0e85096912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image: torch.Tensor):\n",
    "    plt.figure(figsize=(2*len(image),2))\n",
    "    for i in range(len(image)):\n",
    "        plt.subplot(1, len(image), i+1)\n",
    "        image_i = image[i].detach().squeeze()\n",
    "        image_i = F.to_pil_image(image_i)\n",
    "        plt.imshow(np.asarray(image_i))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "def specshow(spec: torch.Tensor):\n",
    "    plt.figure(figsize=(2*len(spec),2))\n",
    "    for i in range(len(spec)):\n",
    "        plt.subplot(1, len(spec), i+1)\n",
    "        spec_i = spec[i].detach().squeeze()\n",
    "        plt.imshow(np.asarray(spec_i))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a234723-4727-403b-824a-c47fad12bf86",
   "metadata": {},
   "source": [
    "## Plot distance matrix\n",
    "\n",
    "The (i, j) component of the distance matrix is the Euclidean distance of the feature between the i-th image and the j-th audio.\n",
    "As can be seen from the plot, the diagonal component of the distance matrix is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef058aae-7c41-4e24-8283-334759ed1a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, image = next(loader)\n",
    "\n",
    "imshow(image)\n",
    "specshow(sound)\n",
    "\n",
    "image, sound = image.to(device), sound.to(device)\n",
    "image, sound = model(image, sound)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(torch.cdist(image, sound).cpu().detach().numpy())\n",
    "plt.title(\"Distance matrix\")\n",
    "plt.xlabel(\"Index of sound\")\n",
    "plt.ylabel(\"Index of image\")\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
