{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07877fe4-bc31-459f-a837-bb4c15f3d416",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Exercise of spoken language acquisition described in the paper:\n",
    "\n",
    "> M. Zhang, T. Tanaka, W. Hou, S. Gao, T. Shinozaki,\n",
    "\"[Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition](http://www.interspeech2020.org/uploadfile/pdf/Thu-2-4-4.pdf),\"\n",
    "in *Proc. Interspeech*, 2020.\n",
    "\n",
    "The details differ slightly from the original paper.\n",
    "\n",
    "## Table of contents\n",
    "1. Definition of food\n",
    "1. Dialogue partner\n",
    "1. SpoLacq environment\n",
    "1. Prepare ASR model\n",
    "1. Make sound dictionary\n",
    "1. RL environment and model creation\n",
    "1. Train RL model\n",
    "1. Save and load RL model\n",
    "1. Retrain RL model\n",
    "1. Test the learnt agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ff50b-72b0-4221-a76b-39c383824508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "import gym\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(\"utils\")\n",
    "\n",
    "from utils.sb3_api import CustomDQNPolicy, CustomDQN\n",
    "from utils.wav2vec2_api import ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c791821-4f08-4022-bd55-8d2611fbe0fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definition of food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e6fdc-e066-4f5c-b62e-50cbf223e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food():\n",
    "    \"\"\"\n",
    "    Definition of food.\n",
    "    It has name, image, and the mean RGB of an image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, image: np.ndarray, RGB: np.ndarray):\n",
    "        self.name = name\n",
    "        self.image = image\n",
    "        self.RGB = RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fc74f-829d-45ce-af34-520aa94855dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dialogue partner\n",
    "\n",
    "<img src=\"../fig/env.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180e9b9-ecdf-42df-b675-070e5f640435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogWorld:\n",
    "    \"\"\"\n",
    "    Dialogue partner for the language learning agent,\n",
    "    which is the outside environment.\n",
    "    An agent receives randomly chosen two images of food from DialogWorld.\n",
    "    The DialogWorld recognizes an agent's utterance and returns feedback.\n",
    "    \n",
    "    :param MAX_STEP: Number of dialogue turns per episode.\n",
    "    :param hasNO: If an agent has the option of not eating either of two foods,\n",
    "        set True.\n",
    "    :param FOODS: Tuple of food names. The type of food names must match\n",
    "        the type of the return value of ASR. For example, Wav2Vec2 returns\n",
    "        a space-delimited sequence of uppercase letters, e.g., GREEN PEPPER.\n",
    "    :param datadir: A directory of food images.\n",
    "    :param asr: If args.use_real_time_asr==True, you can use any ASR of\n",
    "        Callable[[np.ndarray], str]; otherwise, it is the identity function,\n",
    "        i.e., lambda x: x.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        MAX_STEP: int,\n",
    "        hasNO: bool,\n",
    "        FOODS: tuple,\n",
    "        datadir: str,\n",
    "        asr: Callable[[Union[np.ndarray, str]], str],\n",
    "    ):\n",
    "        print(\"Initializaing DialogWorld\")\n",
    "        self.MAX_STEP = MAX_STEP\n",
    "        self.hasNO = hasNO\n",
    "        self.FOODS = FOODS\n",
    "        self.make_food_storage(datadir)\n",
    "        self.asr = asr\n",
    "        print(\"Have prepared ASR\")\n",
    "        self.reset()\n",
    "    \n",
    "    def make_food_storage(self, datadir: str) -> None:\n",
    "        self.food_storage = list() # list of food that the environment has\n",
    "        for f in self.FOODS:\n",
    "            if f == \"NO\": continue\n",
    "            image_paths = glob(f\"{datadir}/{f.replace(' ', '_').lower()}/test*/*.jpg\")\n",
    "            assert len(image_paths) == 30, \"mismatch in test image dataset\"\n",
    "            for image_path in image_paths:\n",
    "                image = Image.open(image_path)\n",
    "                image = image.resize((224, 224))\n",
    "                image = np.array(image)\n",
    "                RGB = np.mean(image.astype(float)/255, axis=(0,1))\n",
    "                self.food_storage.append(Food(f, image, RGB))\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.num = self.MAX_STEP\n",
    "        self.leftfood = random.choice(self.food_storage)\n",
    "        self.rightfood = random.choice(self.food_storage)\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action: Union[np.ndarray, str]) -> Tuple[Tuple[bool, int], bool]:\n",
    "        text = self.asr(action)\n",
    "        if self.hasNO:\n",
    "            correct_answer = [\"NO\", self.leftfood.name, self.rightfood.name]\n",
    "        else:\n",
    "            correct_answer = [self.leftfood.name, self.rightfood.name]\n",
    "        if text in correct_answer:\n",
    "            dlg_success = True\n",
    "            foodID = self.FOODS.index(text)\n",
    "        else:\n",
    "            dlg_success = False\n",
    "            foodID = -1\n",
    "        self.num -= 1\n",
    "        if self.num == 0:\n",
    "            self.done = True\n",
    "        self.leftfood = random.choice(self.food_storage)\n",
    "        self.rightfood = random.choice(self.food_storage)\n",
    "        return (dlg_success, foodID), self.done\n",
    "    \n",
    "    def observe(self) -> dict:\n",
    "        return dict(num=self.num, leftfood=self.leftfood, rightfood=self.rightfood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c24eb9-0e97-4d0e-a81d-dcbb967f01d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SpoLacq environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809064c5-1434-457a-b1df-03eaf4f23fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoLacq1(gym.Env):\n",
    "    \"\"\"\n",
    "    RL environment described in the paper\n",
    "    \n",
    "    M. Zhang, T. Tanaka, W. Hou, S. Gao, T. Shinozaki,\n",
    "    \"Sound-Image Grounding Based Focusing Mechanism for Efficient Automatic Spoken Language Acquisition,\"\n",
    "    in Proc. Interspeech, 2020.\n",
    "    \n",
    "    Internal state of spolacq agent is inside in this environment.\n",
    "    An agent has a preferred color (RGB) as its internal state.\n",
    "    An agent wants food that is close to that color.\n",
    "    An agent receives randomly chosen two images of food from DialogWorld.\n",
    "    An agent is rewarded when it speaks the name of the food it wants.\n",
    "    \n",
    "    :param FOODS: Tuple of food names. The type of food names must match\n",
    "        the type of the return value of ASR. For example, Wav2Vec2 returns\n",
    "        a space-delimited sequence of uppercase letters, e.g., GREEN PEPPER.\n",
    "    :param datadir: A directory of food images.\n",
    "    :param sounddic: If args.use_real_time_asr==True, it is an object of\n",
    "        List[np.ndarray]; otherwise, it is an object of List[str].\n",
    "    :param asr: If args.use_real_time_asr==True, you can use any ASR of\n",
    "        Callable[[np.ndarray], str]; otherwise, it is the identity function,\n",
    "        i.e., lambda x: x.\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RGB = 1 # normalized image\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        FOODS: tuple,\n",
    "        datadir: str,\n",
    "        sounddic: Union[List[np.ndarray], List[str]],\n",
    "        asr: Callable[[Union[np.ndarray, str]], str],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dlgworld = DialogWorld(1, False, FOODS, datadir, asr)\n",
    "        self.action_space = gym.spaces.Discrete(len(sounddic))\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            dict(\n",
    "                state=gym.spaces.Box(low=0, high=self.MAX_RGB, shape=(3,)),\n",
    "                leftfoodRGB=gym.spaces.Box(low=0, high=self.MAX_RGB, shape=(3,)),\n",
    "                rightfoodRGB=gym.spaces.Box(low=0, high=self.MAX_RGB, shape=(3,)),\n",
    "                leftimage=gym.spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.uint8),\n",
    "                rightimage=gym.spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.uint8),\n",
    "                step=gym.spaces.Discrete(self.dlgworld.MAX_STEP+1),\n",
    "                leftfoodID=gym.spaces.Discrete(len(FOODS)),\n",
    "                rightfoodID=gym.spaces.Discrete(len(FOODS)),\n",
    "            )\n",
    "        )\n",
    "        # Read sound files for sound dictionary\n",
    "        self.sounddic = sounddic # convert categorical ID to wave utterance\n",
    "        self.FOODS = FOODS\n",
    "        self.succeeded_log = list()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> dict:\n",
    "        # Initialize the dialogue world\n",
    "        self.dlgworld.reset()\n",
    "        # Initialize the internal state of spolacq agent\n",
    "        self.preferredR = random.random()\n",
    "        self.preferredG = random.random()\n",
    "        self.preferredB = random.random()\n",
    "        return self.observe()\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[dict, int, bool, dict]:\n",
    "        \"\"\":param action: A number in self.action_space\"\"\"\n",
    "        old_state = self.observe()\n",
    "        utterance = self.sounddic[action]\n",
    "        feedback, dlg_done = self.dlgworld.step(utterance)\n",
    "        self.update_internal_state(feedback)\n",
    "        new_state = self.observe()\n",
    "        reward = self.reward(old_state, new_state, feedback)\n",
    "        if reward > 0: self.succeeded_log.append(action)\n",
    "        return new_state, reward, dlg_done, dict()\n",
    "    \n",
    "    def observe(self) -> dict:\n",
    "        insideobs = np.array([self.preferredR, self.preferredG, self.preferredB])\n",
    "        outsideobs = self.dlgworld.observe()\n",
    "        return dict(\n",
    "            state=insideobs,\n",
    "            leftfoodRGB=outsideobs[\"leftfood\"].RGB,\n",
    "            rightfoodRGB=outsideobs[\"rightfood\"].RGB,\n",
    "            leftimage=outsideobs[\"leftfood\"].image,\n",
    "            rightimage=outsideobs[\"rightfood\"].image,\n",
    "            step=outsideobs[\"num\"],\n",
    "            leftfoodID=self.FOODS.index(outsideobs[\"leftfood\"].name),\n",
    "            rightfoodID=self.FOODS.index(outsideobs[\"rightfood\"].name),\n",
    "        )\n",
    "    \n",
    "    def update_internal_state(self, feedback: Tuple[bool, int]) -> None:\n",
    "        # Initialize the internal state of spolacq agent\n",
    "        self.preferredR = random.random()\n",
    "        self.preferredG = random.random()\n",
    "        self.preferredB = random.random()\n",
    "    \n",
    "    def reward(self, old_state: dict, new_state: dict, feedback: Tuple[bool, int]) -> int:\n",
    "        if not feedback[0]: # failed dialogue\n",
    "            return 0\n",
    "        leftfood_distance = np.linalg.norm(old_state[\"state\"] - old_state[\"leftfoodRGB\"])\n",
    "        rightfood_distance = np.linalg.norm(old_state[\"state\"] - old_state[\"rightfoodRGB\"])\n",
    "        if leftfood_distance < rightfood_distance and feedback[1] == old_state[\"leftfoodID\"]:\n",
    "            return 1 # Agent want leftfood and agent's utterance is leftfood's name\n",
    "        elif leftfood_distance >= rightfood_distance and feedback[1] == old_state[\"rightfoodID\"]:\n",
    "            return 1 # Agent want rightfood and agent's utterance is rightfood's name\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def render(self, mode: str = \"console\", close=False) -> None:\n",
    "        state = self.observe()\n",
    "        leftfood_distance = np.linalg.norm(state[\"state\"] - state[\"leftfoodRGB\"])\n",
    "        rightfood_distance = np.linalg.norm(state[\"state\"] - state[\"rightfoodRGB\"])\n",
    "        if leftfood_distance < rightfood_distance:\n",
    "            print(f\"The left {self.FOODS[state['leftfoodID']]} is preferred.\")\n",
    "        else:\n",
    "            print(f\"The right {self.FOODS[state['rightfoodID']]} is preferred.\")\n",
    "        if mode == \"human\":\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(state[\"leftimage\"])\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(state[\"rightimage\"])\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None) -> None:\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e435a-f082-4e84-825c-7275fae9b62f",
   "metadata": {},
   "source": [
    "## Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792d31b-7750-43d5-b7e2-279f76f5badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.datadir = \"data/dataset\"\n",
    "        self.workdir = \"exp1/unsup_rl/test\"\n",
    "        self.load_state_dict_path = self.workdir + \"/unsup_backend.pth.tar\"\n",
    "        self.segment_pkl = self.workdir + \"/seg_audios.txt\"\n",
    "        self.image_nnfeat = self.workdir + \"/data/train_img_fea_sim_8type_clean_limited_data.npy\"\n",
    "        self.segment_nnfeat = self.workdir + \"/data/new720db20k2_fea_sim_8type_clean_limited_data.npy\"\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a627ce-09ee-4583-aec6-40f2024b5b6b",
   "metadata": {},
   "source": [
    "## Food type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab8393-fb35-49f7-b451-869114d0f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOODS = (\n",
    "    \"CHERRY\",\n",
    "    \"GREEN PEPPER\",\n",
    "    \"LEMON\",\n",
    "    \"ORANGE\",\n",
    "    \"POTATO\",\n",
    "    \"STRAWBERRY\",\n",
    "    \"SWEET POTATO\",\n",
    "    \"TOMATO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7642a-6930-4eb3-972b-ed1677bfdbff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare ASR model\n",
    "\n",
    "ASR class is defined at [../utils/wav2vec2_api.py](../utils/wav2vec2_api.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd8fbb-3679-4a45-959d-f16fb9a4836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = ASR(\"facebook/wav2vec2-large-robust-ft-libri-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d126870-f2ed-4efa-bc6b-b2a1fb755d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make sound dictionary\n",
    "\n",
    "<img src=\"../fig/corr.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88406fc5-40b2-4a15-8973-4887aa539d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing mechanism\n",
    "image_features = np.load(args.image_nnfeat).squeeze()\n",
    "segment_features = np.load(args.segment_nnfeat).squeeze()\n",
    "kmeans = KMeans(n_clusters=120, random_state=2).fit(image_features)\n",
    "similarity = -cdist(kmeans.cluster_centers_, segment_features)\n",
    "focused_segment_ids = similarity.argsort(axis=1)[:, -100:].flatten()\n",
    "\n",
    "with open(\"exp1/unsup_rl/test/sounddic.pkl\", \"rb\") as f:\n",
    "    sounddic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4c0dd-740d-4b80-9e98-acacc848d488",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RL environment and model creation\n",
    "\n",
    "Custom policy network is defined at [../utils/sb3_api.py](../utils/sb3_api.py).\n",
    "\n",
    "<img src=\"../fig/dqn.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a6a04-ba83-4499-ab05-230d4b693064",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpoLacq1(FOODS, args.datadir, sounddic, asr)\n",
    "\n",
    "# RL learning model creation\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_kwargs=dict(\n",
    "        cluster_centers=kmeans.cluster_centers_,\n",
    "        load_state_dict_path=args.load_state_dict_path,\n",
    "        num_per_group=100,\n",
    "        purify_rate=0.97,\n",
    "    )\n",
    ")\n",
    "replay_buffer_kwargs = dict(handle_timeout_termination=False)\n",
    "\n",
    "model = CustomDQN(\n",
    "    CustomDQNPolicy,\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    replay_buffer_kwargs=replay_buffer_kwargs,\n",
    "    tensorboard_log=\"./spolacq_tmplog/\",\n",
    "    buffer_size=1000,\n",
    "    learning_starts=50,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffd834-0ec4-4ab1-92d0-9187b876423b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58061c-b30c-4c96-a975-961c30e664ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid CUDA out of memory, total_timesteps is set to 10\n",
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec935e-121a-447a-a359-d8e25472cdb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save RL model and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5c9c3-7d8b-4905-b6fd-7940061cb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(args.workdir+'/dqn')\n",
    "# model.save_replay_buffer(args.workdir+'/replay_buffer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fa954-d83e-4705-baca-884b7c4acaad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load RL model and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4de7e-dfb1-4bde-9c4d-d36349c5b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del env\n",
    "env = SpoLacq1(FOODS, args.datadir, sounddic, asr)\n",
    "model = CustomDQN.load(args.workdir+\"/dqn\", env=env)\n",
    "model.load_replay_buffer(args.workdir+\"/replay_buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c368d58-b3a5-414f-8a42-dbbc5aa3bc19",
   "metadata": {},
   "source": [
    "## Retrain RL model\n",
    "If reset_num_timesteps=False, we can continue the previous tensorboard's log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67bd16-f97a-4ed6-b032-cd8b0d56c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=50000, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c4289-77f1-45ac-bf62-8518b9e92115",
   "metadata": {},
   "source": [
    "## Test the learnt agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cc699-dab8-49d3-b6b4-bafa0f190752",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render(mode=\"human\") # render the state\n",
    "\n",
    "# Agent gets an environment state and returns a decided action\n",
    "action, _ = model.predict(state, deterministic=True)\n",
    "\n",
    "# Environment gets an action from the agent, proceeds the time step,\n",
    "# and returns the new state and reward etc.\n",
    "state, reward, done, info = env.step(action)\n",
    "print(f\"utterance: {asr(sounddic[action])}, reward: {reward}\\n\")\n",
    "Audio(sounddic[action], rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
